---
title: "imbalanced data"
author: "Dr.metales"
date: "1/2/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE, warning=FALSE,error=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

the imbalaced data is the common feature of some type of data such as frodulent credit card where the the number of frodulent cards is usualy very small compared to the number of non frodulent cards. The problem with imblanced data is that the model being trained would be dominated by the majority class such as **knn** models and **svm** models, and hence they would predict the majority class more effectively than the minority class which would result in high rate for sensitivity and low rate for specificity (in binary classification).

One technique to reduce the negative impact of this problem is by subsampling the data. the common subsampling methods used in practice are the following.

* **Upsampling**: this method increases the size of the minority class by sampling with replacement so that the classes will have the same size.

* **Downsampling**: in contrast to the above method, this one decreases the size of the majority class to be the same or closer to the minority class size by just taking out a random sample.

* **Hybrid methods** :  The well known hybrid methods are **ROSE** (Random oversampling examples), and **SMOTE** (Synthetic minority oversampling technique), they downsample the majority class, and creat new artificial points in the minority class. For more detail about **SMOTE** method click [here](https://journals.sagepub.com/doi/full/10.1177/0272989X14560647), and for **ROSE** click [here](https://www.rdocumentation.org/packages/ROSE/versions/0.0-3/topics/ROSE
).

**Note**: all the bove methods should be applied only on the training set , the testing set must be never touched until the final model evaluation step.   

Some type of models can handel imbalanced data such as **deep learning** model with the argument  **class.weight** wich adds more weights to the minority class cases. Other models, however,such as **svm** or **knn** we have to make use of one of the above methods before training these type of models.

In this paper we will make use of the **creditcard** data from kaggle website -click [here](https://www.kaggle.com/arvindratan/creditcard#creditcard.csv) to upload this data, which is highly imblanced- and we will train a **logistic regression** model on the raw data and on the transformed data after applying the above methods and comparing the results. Also, we will use a simple deep learning model with and without taking into account the imbalanced problem.    

First we call the data. 

```{r, message=FALSE}
library(tidyverse)
data<-read.csv("creditcard.csv",header=TRUE)
```


For confidance purposes the original features are replaced by the PCA variables from v1 to v28 and only **Time** and **Amount** features that are left from the original features. 

Let's first check **Class** variable levels frequency (after having been converted to a factor type).

```{r}
data$Class<-as.factor(data$Class)
prop.table(table(data$Class))
```

As we see the minority class number "1" is only about 0.17% of the data number.

We also need to show the summary of the data to take an overall look at all the features to be aware of missing values or unusual outliers.

```{r}
summary(data)
```

looking at this summary, we should standardize the features since they have a different magnitude values.

## Data partition

Before applying any subsampling method we split the data first between the training set and the testing set and we use only the former to be subsampled.


```{r, message=FALSE}
library(caret)
set.seed(1234)
index<-createDataPartition(data$Class,p=0.8,list=FALSE)
train<-data[index,]
test<-data[-index,]
```

Now we make use of the above methods to transform the training set.

## Subsampling the training set


* **Upsampling** : we make use of the caret function **upSample** as follows

```{r}
set.seed(111)

trainup<-upSample(x=train[,-ncol(train)],
                  y=train$Class)

table(trainup$Class)
```


Now the two classes have the same size **227452**

* **downsampling**: we use the caret function **downSample**


```{r}
set.seed(111)
traindown<-downSample(x=train[,-ncol(train)],
                  y=train$Class)

table(traindown$Class)
```

now the size of each class is **394**


* **ROSE**: to use this technique we have to call the **ROSE** package

```{r}
library(ROSE)
set.seed(111)

trainrose<-ROSE(Class~.,data=train)$data

table(trainrose$Class)
```

since this technique add new synthetic data points to the minority class and daownsamples the majority class the size now is about **114019** for minority class and **113827** for the majority class.

* **SMOTE**: this technique requires the **DMwR** package.


```{r}
library(DMwR)
set.seed(111)

trainsmote <- SMOTE(Class~.,data = train)



table(trainsmote$Class)
```

The size of the majority class is **113827** and for the minority class is **114019** .


## training logistic regression model.

we are now ready to fit logit model to the original training set without subsampling, and to each of the above subsampled training sets.

### original train set


```{r}
set.seed(123)
model <- glm(Class~., data=train, family = "binomial")
                
summary(model)
```

Next we remove the insignificant variables. 

```{r}
set.seed(123)
model1 <- glm(Class~.-Time-V2-V3-V6-V7-V9-V11-V12-V15-V16-V17-V18-V19-V24-V25-V26, data=train, family = "binomial")
                
summary(model1)
```

We have now two predictors that are non significant  **V1** and **Amount**, they should be laso removed.


```{r}
set.seed(123)
finalmodel <- glm(Class~.-Time-V1-V2-V3-V6-V7-V9-V11-V12-V15-V16-V17-V18-V19-V24-V25-V26-Amount, data=train, family = "binomial")
                
summary(finalmodel)
```

For the other training sets we will use only these significant predictors from the above model.

Now let's get the final results from the confusion matrix.


```{r}
pred <- predict(finalmodel,test, type="response")
pred <- as.integer(pred>0.5)
confusionMatrix(as.factor(pred),test$Class, positive = "1")

```
 As we see we have a large accuracy rate about **99.92%**. However, if we look at the sensitivity rate which is about **58.16%** indicating that the model poorly predict the frodulent cards which is the most important class label that we want to predict correctly, unlike the class label "0" wich has a high accuracy rate **99.98%**.  
 
This large diference in predicting the class labels is reflected by the high imbalance rate.  

### Upsampling the train set 


```{r}
set.seed(123)
modelup <- glm(Class~V4+V5+V8+V10+V13+V14+V20+V21+V22+V23+V27+V28, data=trainup, family = "binomial")
                
summary(modelup)
```




```{r}
pred <- predict(modelup,test, type="response")
pred <- as.integer(pred>0.5)
confusionMatrix(as.factor(pred),test$Class, positive = "1")

```



Now we have a smaller accuracy rate **97.29%**, but we have a larger  sensitivity rate **87.75%** which increases the power of the model to predict the frodulent cards.


### Down sampling the training set.



```{r}
set.seed(123)
modeldown <- glm(Class~V4+V5+V8+V10+V13+V14+V20+V21+V22+V23+V27+V28, data=traindown, family = "binomial")
pred <- predict(modeldown,test, type="response")
pred <- as.integer(pred>0.5)
confusionMatrix(as.factor(pred),test$Class, positive = "1")
                
```

Approximatly we get the same sensitivity rate **87.75%** with a slight decrease of the over all accuracy rate **96.42%**, and the specificity rate hes decreased to **96.43%** since we have decreased the majority class size by downsampling.

### subsampline the train set by ROSE technique


```{r}
set.seed(123)
modelrose <- glm(Class~V4+V5+V8+V10+V13+V14+V20+V21+V22+V23+V27+V28, data=trainrose, family = "binomial")
pred <- predict(modelrose,test, type="response")
pred <- as.integer(pred>0.5)
confusionMatrix(as.factor(pred),test$Class, positive = "1")
                
```

Using this method the sensitivity rate is slightly smaller than the previous ones **85.71%** but still a large improvment in predicting frodulent cards compared to the model trained with the original impbalanced data.


### Subsampling the train set by SMOTE technique


```{r}
set.seed(123)
modelsmote <- glm(Class~V4+V5+V8+V10+V13+V14+V20+V21+V22+V23+V27+V28, data=trainsmote, family = "binomial")
pred <- predict(modelsmote,test, type="response")
pred <- as.integer(pred>0.5)
confusionMatrix(as.factor(pred),test$Class, positive = "1")
                
```

With this method we get the same sensitivity rate **85.71%** such as ROSE method.


## deep learning model.

 For this model we call **keras** package.
to train this model we should first convert the data (train and test sets) into numeric matrix and remove the collumn names (we convert also the **Class** to numeric type).

```{r}

library(keras)
train$Class<-as.numeric(train$Class)
test$Class<-as.numeric(test$Class)
train[,ncol(train)]<-train[,ncol(train)]-1
test[,ncol(test)]<-test[,ncol(test)]-1
trained<-as.matrix(train)
dimnames(trained)<-NULL
tested<-as.matrix(test)
dimnames(tested)<-NULL

```



Now we pull out our target variable **Class**

```{r}

trainy<-trained[,ncol(trained)]
testy<-tested[,ncol(tested)]
trainx<-trained[,-ncol(trained)]
testx<-tested[,-ncol(tested)]
```

then we apply one hot encoding on the target variable. 

```{r}
trainlabel<-to_categorical(trainy)
testlabel<-to_categorical(testy)
```

The final step now is normalizing the matrices (trainx and testx)

```{r}
trainx<-normalize(trainx)
testx<-normalize(testx)
```

Now we are ready to creat the model with one hidden layer.


```{r}
modeldeep <- keras_model_sequential()

modeldeep %>%
    layer_dense(units=60,activation = "relu",
              kernel_initializer = "he_normal",input_shape =c(30))%>%
  layer_dropout(rate=0.2)%>%
  layer_dense(units=50,activation = "relu",
              kernel_initializer = "he_normal")%>%
  layer_dropout(rate=0.1)%>%
        layer_dense(units=2,activation = "sigmoid")

summary(modeldeep)
```

For imbalanced data is recomende to use **kappa** metric instead of accuracy  

```{r}
modeldeep %>%
  compile(loss="binary_crossentropy",
          optimizer="adam",
          metric="accuracy")
```


Now let's excute the model



```{r}
#history<- modeldeep %>%
  #fit(trainx,trainlabel,batch_size=5,validation_split=0.2)
  
```


Now we save the model than we reload it to save time when kniting the file. 


```{r}
#save_model_hdf5(modeldeep,"modeldeep.h5")
modeldeep<-load_model_hdf5("modeldeep.h5")

```



What we care more about is the test set.



```{r}
pred<-  modeldeep %>%
  predict_classes(testx)
confusionMatrix(as.factor(pred),as.factor(testy))

```


As we see this model predicts all casses as class label "0" because this label dominates the whole data.
 

### deep learning model with class weights



Now let's try the previous model by taking into account the class imbalance


```{r}
modeldeep1 <- keras_model_sequential()

modeldeep1 %>%
    layer_dense(units=60,activation = "relu",
              kernel_initializer = "he_normal",input_shape =c(30))%>%
  layer_dropout(rate=0.2)%>%
  layer_dense(units=50,activation = "relu",
              kernel_initializer = "he_normal")%>%
  layer_dropout(rate=0.1)%>%
        layer_dense(units=2,activation = "sigmoid")

modeldeep1 %>%
  compile(loss="binary_crossentropy",
          optimizer="adam",
          metric="accuracy")

```


Now let's excute the model



```{r}
#history1<- modeldeep1 %>%
  #fit(trainx,trainlabel,batch_size=5,  validation_split=0.2,class_weight=list("0"=1,"1"=577))


```



```{r}
#save_model_hdf5(modeldeep1,"modeldeep1.h5")
modeldeep1<-load_model_hdf5("modeldeep1.h5")

```


Now let's get the confusion matrix.


```{r}
pred<-  modeldeep1 %>%
  predict_classes(testx)
confusionMatrix(as.factor(pred),as.factor(testy))

```


the situation now reversed towards the class label "1" with specificity rate **100%** , although this model has very poor overall accuracy rate **0.9%** but it predicts  perfectly the frodulent cards and actually this is what we want to predict.
 

## Conclusion

with the imbalanced data most machine learning model tend to more effeciently predict the majority class than the minority class. To correct thus this behaviour we can use one of the above dicussed methods to get more closer accuarcy rates between classes. However, deep learning model can easily handel this problem by specofying the class weights. 

